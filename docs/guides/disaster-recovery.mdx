---
title: Disaster Recovery
description: Learn how to set up disaster recovery for ReductStore to ensure data resilience and availability.
---

import CodeBlock from "@theme/CodeBlock";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

<head>
  <link
    rel="canonical"
    href="https://www.reduct.store/docs/guides/disaster-recovery"
  />
</head>

# Disaster Recovery

This guide explains how to set up disaster recovery for ReductStore to ensure data resilience and availability.
It covers the following types of disasters:

1. **Hardware Failure**: Physical damage to the server or storage devices.
2. **Data Corruption**: Logical errors that lead to data being unreadable or inconsistent.
3. **Network Outage**: Loss of connectivity that prevents access to the ReductStore instance.
4. **Operational Errors**: Mistakes made by users or administrators that lead to data loss or corruption.

| Disaster Recovery Strategy                 | Description                                                                                      | Downtime      | Hardware Failure | Data Corruption | Network Outage | Operational Errors |
|--------------------------------------------|--------------------------------------------------------------------------------------------------|---------------|------------------|-----------------|----------------|--------------------|
| Data Loss Detection and Automated Recovery | Mechanisms to detect data loss and recover automatically to keep an instance working             | seconds       | ✅ (limited)      | ✅ (limited)     | ❌              | ❌                  |
| Backup/Restore Strategy                    | Regular backups and restore processes to recover from data loss or corruption                    | minutes/hours | ✅                | ✅               | ❌              | ✅                  |
| Pilot light Instance                       | Prepared second instance to switch the HTTP traffic when the primary instance is not available    | minutes       | ✅                | ❌               | ✅              | ❌                  |
| Multi-site Active-Active Setup             | Multiple ReductStore instances in different locations to ensure high availability and redundancy | zero          | ✅                | ✅               | ✅              | ✅                    |


## Data Loss Detection And Automated Recovery

ReductStore’s disaster recovery strategy begins with detecting data loss. The core principle is: whatever happens, detect and isolate the problem as early as possible, and continue operating with unaffected data.

Detection and isolation are based on the internal storage structure and format, which you can learn more about in the **[How Does It Work](../how-does-it-work#blocks)** section.
These mechanisms operate during both instance startup and I/O operations.

### Startup Data Loss Detection

When a ReductStore instance starts, the storage engine performs the following checks:

- Verifies and replays Write-Ahead Logs (WALs)
- Updates indexes
- Validates data integrity

This process helps recover from the following scenarios:

1. **Power Failure or Crash**:

- If ReductStore was not shut down properly, the engine detects incomplete WALs and restores the data to the last consistent state.

2. **File System Corruption**:

- Each index file has a CRC to verify its integrity. If corrupted, the engine will detect this and recreate it from block descriptors.
- If the number or size of data blocks doesn’t match the index file, it will be rebuilt using the block descriptors.
- If the storage engine detects a corrupted block descriptor during the index rebuild, it will remove the corrupted block and continue with the rest of the data.

### Runtime Data Loss Detection

During I/O operations, ReductStore checks the integrity of block descriptors.
If corruption is detected:

- The storage engine aborts the current I/O operation.
- A `500 Internal Server Error` is returned.
- The issue is logged for administrative review.

An administrator can inspect the logs and take corrective actions such as removing the corrupted block or restoring it from a backup.

:::note
Not every I/O operation will trigger a check. The checks are preformed when a block descriptor is read from disk to an in-memory cache.
:::

### Content Integrity

The detection mechanisms described above ensure the integrity of metadata and block descriptors—essential for the reliable functioning of the storage engine.

However, they **do not verify the actual content** of stored records, to avoid performance penalties.
To ensure content-level integrity, users can implement their own checksum or hash validation and attach them as labels to records:

```python
md5_hash = md5(data).hexdigest()
ts = time.time()
await bucket.write(
    "entry_name",
    data,
    timestamp=ts,
    labels={
        "md5": md5_hash,
    },
)
```

## Backup/Restore Strategy

The backup/restore strategy is crucial for disaster recovery, especially for scenarios where data corruption or loss occurs.
The strategy involves creating regular backups of the ReductStore instance and restoring data from these backups when necessary.

### Backup Process

A backup can be created by copying the entire ReductStore data directory (RS_DATA_PATH variable), which contains all the necessary files for recovery
configuration and data.

To make the backup process more efficient, you should do the following:

1. Shutdown the ReductStore instance to ensure data consistency.
2. Create a backup of the entire data directory.
3. Restart the ReductStore instance.

A backup must be tested after creation to ensure it can be restored successfully. To test a backup, you need to restore it to a separate instance and verify that the data is intact and accessible.

1. Copy the backup to a new directory.
2. Run a new ReductStore instance with the copied data directory `RS_DATA_PATH=/path/to/backup reductstore`
3. Verify logs for any errors during startup.
4. Verify that all expected data is recovered and accessible: `curl http://127.0.0.1:8383/api/v1/list`

### Restore Process

Buckups can be restored by copying the backup data directory to the original ReductStore data directory.
You can restore buckups fully or partially, depending on your needs:

1. **Full Restore**: Restores all data and configuration from the backup.
2. **Bucket Restore**: Restores data and configuration for a specific bucket.
3. **Entry Restore**: Restores data for a specific entry within a bucket.

#### Full Restore

To perform a full restore, follow these steps:

1. Stop the ReductStore instance.
2. Remove the current data directory or rename it for backup.
3. Copy the backup data directory to the original ReductStore data directory.
4. Start the ReductStore instance.

#### Bucket Restore

To restore a specific bucket, follow these steps:

1. Stop the ReductStore instance.
2. Remove the bucket directory from the current data directory. It is located at `RS_DATA_PATH/<bucket_name>`.
3. Copy the backup bucket directory to the original ReductStore data directory.
4. Start the ReductStore instance.

#### Entry Restore

To restore a specific entry within a bucket, follow these steps:

1. Stop the ReductStore instance.
2. Remove the entry file from the current bucket directory. It is located at `RS_DATA_PATH/<bucket_name>/<entry_name>`.
3. Copy the backup entry file to the original bucket directory.
4. Start the ReductStore instance.

## Pilot Light

A pilot light instance is a secondary ReductStore instance that is prepared to take over HTTP traffic when the primary instance is unavailable.
This strategy ensures minimal downtime and quick recovery in case of a network outage or hardware failure.

![Pilot Light Instance](./img/disaster_recovery_pilot_light.png)


To implement this strategy, you will need additional componets:

1. **Load Balancer**: A load balancer to route HTTP traffic between the primary and pilot light instances.
2. **Monitoring System**: A monitoring system to detect when the primary instance is down
3. **Data Replication Tool**: A tool to replicate data between the primary and pilot light instances.

### Load Balancer

A load balancer is used to distribute HTTP traffic between the primary and pilot light instances.
It should be configured to route traffic to the pilot light instance when the primary instance is down.

### Monitoring System

A monitoring system is essential to detect when the primary instance is down and to trigger the switch to the pilot light instance.
It should monitor HTTP traffic for 5xx errors and connectivity and notify when the primary instance is unavailable.

### Data Replication

A data replication tool is used to keep the pilot light instance up-to-date with the primary instance.
The tool must be chosen based on the type of storage used by the ReductStore instance.


### Switching to Pilot Light

The switch to the pilot light instance can be manual or automatic, depending on the monitoring system and load balancer configuration.
When the primary instance is detected as unavailable, the load balancer should route traffic to the pilot light instance.

Independently of the switch is made manually or automatically, it must be done in the following order:

1. Ensure that the primary instance is stopped to prevent data corruption. (if it is still running)
2. Make sure that data replication is up-to-date and the pilot light instance has the latest data.
3. Launch the pilot light instance and ensure it is running.


## Active-Active Setup

An ctive-active setup involves deploying multiple ReductStore instances and replicating data between them.
This strategy ensures high availability, redundancy, and quick disaster recovery however it requires more resources and supporting infrastructure.

![Active-Active Setup](./img/disaster_recovery_active_active.png)

The active-active setup consists of two layers:

1. **Ingress Layer**: A load balancer and ReductStore instances with high performance but small storage capacity to ingest data and predicate it to the egress layer.
2. **Egress Layer**: A load balancer and ReductStore instances with high storage capacity to store data and serve queries.

Each layer has its own endpoints and users must use ingress endpoints to write data and egress endpoints to read data.

### Ingress Layer

The ingress layer is responsible for ingesting data and replicating it to the egress layer.
Each ingress instance has the same configuration and replication tasks to copy data to each egress instance.
It has a small storage capacity only for buffering incoming data before it is replicated to the egress layer.

When a client writes data, the load balancer routes the request to one of the ingress instances. Because all ingress instances have the same configuration, the data is replicated to all egress instances and
each egress instance has the same data.

### Egress Layer

The egress layer is responsible for storing data and serving queries. Each egress instance has the same configuration and data.

When a client queries data, the load balancer routes the request to one of the egress instances.

### Configuration

ReductStore replicates data between instances but doesn't sync their configurations. Due to the multi-site setup needs the same configuration for all instances,
you should provision each bucket and replication task with IaC tools like Terraform or Ansible, or use ReductStore CLI or SDK to automate the configuration process.

### Load Balancing Requirements

The load balancing must be configured to route traffic from a client to the same instance for both write and read operations.
This necessary for the following reasons:

1. HTTP Query API works as an iterator and needs multiple requests to read all data. The load balancer must route all requests to the same instance to ensure that the iterator works correctly.
2. ReductStore offers the best performance when the client writer records in time order. If the load balancer routes requests to different instances, the data may be written out of order, which can lead to performance degradation.
